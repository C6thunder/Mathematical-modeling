import numpy as np
import matplotlib.pyplot as plt
from scipy.integrate import solve_ivp, odeint
from scipy.special import erfc
import pandas as pd
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.preprocessing import StandardScaler
import xgboost as xgb

# å®šä¹‰è¯ç‰©åŠ¨åŠ›å­¦æ¨¡å‹ï¼ˆä¾‹å¦‚ï¼ŒåŒå±‚çš®è‚¤æ¨¡å‹ï¼‰
def model(t, y, params):
    Cs_S, Cs_L = y
    k1, k2, k3 = params  # å‡è®¾æœ‰3ä¸ªå…³é”®å‚æ•°ï¼šçš®è‚¤ä¸Šå±‚æ¸—é€ç‡ï¼Œçš®è‚¤ä¸‹å±‚æ¸—é€ç‡ï¼Œè¯ç‰©æ¶ˆé™¤é€Ÿç‡
    dCs_S_dt = -k1 * Cs_S
    dCs_L_dt = k1 * Cs_S - k2 * Cs_L
    return [dCs_S_dt, dCs_L_dt]

# å‚æ•°è®¾å®š
params = [0.1, 0.05, 0.01]  # å‡è®¾çš®è‚¤æ¸—é€ç‡è¾ƒä½ï¼Œå°è¯•è°ƒæ•´è¿™äº›å‚æ•°

# åˆå§‹æ¡ä»¶ï¼šå‡è®¾çš®è‚¤è¡¨é¢æœ‰å°‘é‡è¯ç‰©
initial_conditions = [1, 0]  # çš®è‚¤è¡¨é¢æµ“åº¦åˆå§‹ä¸º1ï¼Œçš®è‚¤ä¸‹å±‚æµ“åº¦ä¸º0

# æ—¶é—´åŒºé—´è®¾å®š
t_span = (0, 10)  # ä»0åˆ°10å°æ—¶
t_eval = np.linspace(0, 10, 100)  # æ—¶é—´æ­¥é•¿100

# ä½¿ç”¨solve_ivpè¿›è¡Œæ±‚è§£
solution = solve_ivp(model, t_span, initial_conditions, args=(params,), t_eval=t_eval, method='RK45')

# è¾“å‡ºç»“æœ
plt.plot(solution.t, solution.y[0], label='Cs_S (çš®è‚¤ä¸Šå±‚æµ“åº¦)')
plt.plot(solution.t, solution.y[1], label='Cs_L (çš®è‚¤ä¸‹å±‚æµ“åº¦)')
plt.xlabel('æ—¶é—´ (å°æ—¶)')
plt.ylabel('è¯ç‰©æµ“åº¦ (ng/mL)')
plt.legend()
plt.show()

# Predict dose by regression function (example)
def dose_formula(BMI, L_skin):
    return 8 + 0.3 * BMI + 15 * L_skin

print("Recommended patch area:", dose_formula(22, 0.1), "cmÂ²")

# =====================
# 6. ä¸´åºŠæ•°æ®åˆ†æä¸æœºå™¨å­¦ä¹ å»ºæ¨¡
# =====================

df = pd.read_csv("clinical_trial_data.csv")

# Clean data
cols_to_clean = ['è¡€è¯æµ“åº¦(ng/mL)', 'çš®è‚¤è¡¨å±‚æµ“åº¦(Î¼g/cm2)', 'çš®è‚¤æ·±å±‚æµ“åº¦(Î¼g/cm2)']
for col in cols_to_clean:
    if col in df.columns:
        df[col] = df[col].astype(str).str.extract(r'([\d.]+)').astype(float)

df = df.dropna()
df.to_csv("cleaned_data.csv", index=False)
df.to_excel("cleaned_data.xlsx", index=False)

print("\nğŸ“Š æè¿°ç»Ÿè®¡ï¼š")
print(df.describe())

print("\nğŸ“ˆ ç›¸å…³æ€§çŸ©é˜µï¼š")
correlation = df.corr(numeric_only=True)
print(correlation)

sns.heatmap(correlation, annot=True, cmap="coolwarm")
plt.title("ç›¸å…³æ€§çƒ­å›¾")
plt.tight_layout()
plt.show()

columns_to_plot = ['å¹´é¾„', 'BMI', 'çš®è‚¤åšåº¦(mm)', 'è¡€è¯æµ“åº¦(ng/mL)', 'å‰¯ä½œç”¨è¯„åˆ†(1-5)']
available_columns = [col for col in columns_to_plot if col in df.columns]
if len(available_columns) >= 2:
    sns.pairplot(df[available_columns])
    plt.suptitle("æ•£ç‚¹å›¾çŸ©é˜µ", y=1.02)
    plt.show()

# =====================
# æœºå™¨å­¦ä¹  - XGBoostå¢å¼ºåˆ†ç±»æ¨¡å‹
# =====================

features = ['å¹´é¾„', 'BMI', 'çš®è‚¤åšåº¦(mm)', 'ç»™è¯æ—¶é—´(h)', 'è¡€è¯æµ“åº¦(ng/mL)', 
            'çš®è‚¤è¡¨å±‚æµ“åº¦(Î¼g/cm2)', 'çš®è‚¤æ·±å±‚æµ“åº¦(Î¼g/cm2)']
features = [f for f in features if f in df.columns]

if 'å‰¯ä½œç”¨è¯„åˆ†(1-5)' in df.columns and len(features) > 1:
    X = df[features]
    y = df['å‰¯ä½œç”¨è¯„åˆ†(1-5)']

    # Standardize features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # Adjust y labels to start from 0
    y = y - 1  # This will make the classes start from 0

    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=42)

    # XGBoost model (ensure num_class matches the target variable classes)
    xgb_model = xgb.XGBClassifier(objective='multi:softmax', num_class=5, random_state=42)

    # Hyperparameter tuning using GridSearchCV
    param_grid = {
        'max_depth': [3, 6, 10],
        'learning_rate': [0.01, 0.05, 0.1],
        'n_estimators': [100, 200],
        'subsample': [0.8, 1.0]
    }

    grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)
    grid_search.fit(X_train, y_train)

    # Best parameters found by GridSearchCV
    print(f"Best parameters found: {grid_search.best_params_}")
    
    # Evaluate on the test set
    best_xgb_model = grid_search.best_estimator_
    y_pred = best_xgb_model.predict(X_test)

    print("\nğŸ§ª åˆ†ç±»æŠ¥å‘Šï¼š")
    print(classification_report(y_test, y_pred))

    print("\nğŸ§± æ··æ·†çŸ©é˜µï¼š")
    print(confusion_matrix(y_test, y_pred))

    # Calculate accuracy
    accuracy = accuracy_score(y_test, y_pred)
    print(f"\nğŸ“Š æ¨¡å‹ç²¾åº¦ï¼š{accuracy * 100:.2f}%")

    importances = best_xgb_model.feature_importances_
    sorted_idx = np.argsort(importances)

    plt.barh(range(len(sorted_idx)), importances[sorted_idx], align="center")
    plt.yticks(range(len(sorted_idx)), [features[i] for i in sorted_idx])
    plt.title("ç‰¹å¾é‡è¦æ€§")
    plt.tight_layout()
    plt.show()
